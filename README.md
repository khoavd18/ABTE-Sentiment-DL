ABTE-Sentiment-DL

EN / VI bilingual README ‚Äî Full project README in both English and Ti·∫øng Vi·ªát.
Quick language index:

English

Ti·∫øng Vi·ªát

English
Overview

ABTE-Sentiment-DL implements a complete pipeline for:

Aspect-Based Term Extraction (ABTE) at token level with BIO tags (B-Term, I-Term, O).

Sentence-level Sentiment Classification with 3 classes: Negative / Neutral / Positive.

Supported models:

üß± CNN 1D (Conv1D)

üåÄ LSTM

üî∑ Transformer Encoder (custom)

ü§ó Fine-tune DistilBERT for TokenClassification (ABTE)

A Streamlit demo is included. It auto-detects .safetensors checkpoints (safe to load without Torch 2.6) and highlights aspects + predicts sentence/aspect polarities.

Datasets

Fill with your actual numbers/links if different.

Dataset	Task	Classes	Train	Test
ABTE-Restaurants	ABTE (BIO)	3	3,602	1,119
Sentiment-Tweets	Sentiment (3c)	3	40,000	10,000

ABTE: columns Tokens (list[str]), Tags (list[int] with O=0, B=1, I=2).

Sentiment: columns text, label (0/1/2 ‚áî Neg/Neu/Pos).

Replace with your sources/links if needed.

Settings

Default unless specified:

Epochs: 20‚ÄÉ(Early Stopping patience=3)

Batch size: 256 (train/eval)

Weight decay: 0.01

Optimizers & LR

CNN/LSTM/Transformer: Adam(lr=2e-3)

DistilBERT fine-tuning: AdamW(lr=5e-5)

Tokenizer: WordLevel (or BPE), trained on train split if applicable

Checkpoint: save_safetensors=True ‚Üí produces model.safetensors

1 epoch = full pass over data (each sample used exactly once)

Installation
python -m venv .venv
# Windows PowerShell
.venv\Scripts\activate
pip install -U -r requirements.txt


requirements.txt (suggested)

streamlit>=1.37
transformers>=4.41
tokenizers>=0.15
torch>=2.0
evaluate>=0.4
seqeval==1.2.2


Security note (CVE-2025-32434): this project uses safetensors for weights, so Torch 2.6+ is not required to run inference.

Training
Quick commands per model
# CNN 1D
python train.py --model conv1d        --output_dir out_cnn

# LSTM
python train.py --model lstm          --output_dir out_lstm

# Transformer encoder (custom)
python train.py --model transformer   --output_dir out_transformer

# DistilBERT fine-tuning for ABTE
python train.py --model finetune      --output_dir out_finetune \
  --pretrained distilbert/distilbert-base-uncased

Custom parameters
python train.py --model conv1d \
  --epochs 20 --batch_size 256 --lr 2e-3 --patience 3 \
  --output_dir out_cnn


If you only have one CSV for sentiment, the script splits 80/20 with stratify=label.

Experiments

Hardware examples (edit as needed):

GPU: RTX 4090 24GB / TITAN X 12GB

VRAM guide: CNN/LSTM/Transformer ~ 1‚Äì3 GB; DistilBERT fine-tune ~ 6‚Äì8 GB

Example test results (replace with your numbers):

Task	Size	Model	F1 (reproduced)	Ref
ABTE	Small	CNN 1D	0.xx	‚Äì
ABTE	Small	LSTM	0.xx	‚Äì
ABTE	Small	Transformer Encoder	0.xx	‚Äì
ABTE	Small	DistilBERT (ft)	0.xx	‚Äì

Detailed config/logs per run: out_<model>/logs.txt

Checkpoints/state: out_<model>/checkpoint-XXXX/

Curves (F1 / Loss)

Generate curves from trainer_state.json:

python draw.py


Outputs:

compare_f1.png ‚Äî eval/F1 vs. epoch

compare_loss.png ‚Äî eval/loss vs. epoch

Embed after committing:

![F1 curves](./compare_f1.png)
![Loss curves](./compare_loss.png)

Streamlit Demo

Run directly:

streamlit run app.py


The app auto-detects the latest model.safetensors (prioritizes out_finetune/checkpoint-*).

To pin a directory:

# inside app.py
ABTE_DIR = "model_abte"  # folder with model.safetensors + tokenizer files


Minimal inference bundle (keep):

model.safetensors, config.json

tokenizer.json, tokenizer_config.json, special_tokens_map.json

(vocab.txt / merges.txt / spiece.model depending on tokenizer)

Remove from repo:

optimizer.pt, scheduler.pt, scaler.pt, rng_state.pth, trainer_state.json,
training_args.bin, any legacy .bin/.pt not needed at inference

Project Structure
ABTE-Sentiment-DL/
‚îú‚îÄ app.py
‚îú‚îÄ train.py
‚îú‚îÄ models.py
‚îú‚îÄ init.py
‚îú‚îÄ draw.py
‚îú‚îÄ requirements.txt
‚îú‚îÄ out_cnn/
‚îú‚îÄ out_lstm/
‚îú‚îÄ out_transformer/
‚îú‚îÄ out_finetune/
‚îî‚îÄ model_abte/
   ‚îú‚îÄ model.safetensors
   ‚îú‚îÄ config.json
   ‚îú‚îÄ tokenizer.json
   ‚îú‚îÄ tokenizer_config.json
   ‚îú‚îÄ special_tokens_map.json
   ‚îî‚îÄ (vocab.txt | merges.txt | spiece.model)

Deployment Notes

Security: use safetensors for checkpoints and use_safetensors=True for loading.

Large Git pushes (HTTP 408): migrate large files to Git LFS
git lfs migrate import --include="*.safetensors,*.pt,*.pth,*.bin,*.h5" --everything
or host the model on Hugging Face Hub and keep GitHub code-only.

.gitignore (suggested)

__pycache__/
.venv/
**/optimizer.pt
**/scheduler.pt
**/scaler.pt
**/rng_state.pth
**/trainer_state.json
**/training_args.bin
**/pytorch_model.bin
**/*.pt
!**/model.safetensors

License

MIT or Apache-2.0 (choose one and include a LICENSE file).

Acknowledgements

Hugging Face (Transformers/Datasets/Evaluate), SeqEval, and the public datasets used.

Ti·∫øng Vi·ªát
Gi·ªõi thi·ªáu

ABTE-Sentiment-DL hi·ªán th·ª±c tr·ªçn v·∫πn:

Tr√≠ch aspect (ABTE) ·ªü m·ª©c token (nh√£n BIO: B-Term, I-Term, O).

Ph√¢n lo·∫°i c·∫£m x√∫c c√¢u v·ªõi 3 l·ªõp: Ti√™u c·ª±c / Trung t√≠nh / T√≠ch c·ª±c.

H·ªó tr·ª£ c√°c m√¥ h√¨nh:

üß± CNN 1D (Conv1D)

üåÄ LSTM

üî∑ Transformer Encoder (t√πy bi·∫øn)

ü§ó Fine-tune DistilBERT cho TokenClassification (ABTE)

C√≥ demo Streamlit t·ª± d√≤ checkpoint .safetensors, t√¥ m√†u aspect v√† d·ª± ƒëo√°n c·∫£m x√∫c c√¢u/cho t·ª´ng aspect.

D·ªØ li·ªáu

ƒêi·ªÅn s·ªë li·ªáu/ƒë∆∞·ªùng d·∫´n n·∫øu kh√°c.

Dataset	B√†i to√°n	S·ªë l·ªõp	Train	Test
ABTE-Restaurants	ABTE (BIO)	3	3,602	1,119
Sentiment-Tweets	C·∫£m x√∫c (3c)	3	40,000	10,000

ABTE: c·ªôt Tokens (list[str]), Tags (list[int] v·ªõi O=0, B=1, I=2).

Sentiment: c·ªôt text, label (0/1/2 ‚áî Ti√™u c·ª±c/Trung t√≠nh/T√≠ch c·ª±c).

Thay b·∫±ng ngu·ªìn/link d·ªØ li·ªáu c·ªßa b·∫°n n·∫øu c·∫ßn.

Thi·∫øt l·∫≠p

M·∫∑c ƒë·ªãnh (tr·ª´ khi n√™u kh√°c):

Epochs: 20‚ÄÉ(Early Stopping patience=3)

Batch: 256 (train/eval)

Weight decay: 0.01

T·ªëi ∆∞u & LR

CNN/LSTM/Transformer: Adam(lr=2e-3)

Fine-tune DistilBERT: AdamW(lr=5e-5)

Tokenizer: WordLevel (ho·∫∑c BPE)

Checkpoint: save_safetensors=True ‚Üí sinh model.safetensors

1 epoch = duy·ªát h·∫øt d·ªØ li·ªáu (m·ªói m·∫´u d√πng ƒë√∫ng 1 l·∫ßn)

C√†i ƒë·∫∑t
python -m venv .venv
# Windows PowerShell
.venv\Scripts\activate
pip install -U -r requirements.txt


requirements.txt (g·ª£i √Ω)

streamlit>=1.37
transformers>=4.41
tokenizers>=0.15
torch>=2.0
evaluate>=0.4
seqeval==1.2.2


Ghi ch√∫ b·∫£o m·∫≠t: d·ª± √°n d√πng safetensors, kh√¥ng c·∫ßn n√¢ng Torch 2.6+ ƒë·ªÉ suy lu·∫≠n.

Hu·∫•n luy·ªán
L·ªánh nhanh theo m√¥ h√¨nh
# CNN 1D
python train.py --model conv1d        --output_dir out_cnn

# LSTM
python train.py --model lstm          --output_dir out_lstm

# Transformer encoder
python train.py --model transformer   --output_dir out_transformer

# Fine-tune DistilBERT cho ABTE
python train.py --model finetune      --output_dir out_finetune \
  --pretrained distilbert/distilbert-base-uncased

Tu·ª≥ bi·∫øn tham s·ªë
python train.py --model conv1d \
  --epochs 20 --batch_size 256 --lr 2e-3 --patience 3 \
  --output_dir out_cnn


N·∫øu b·∫°n ch·ªâ c√≥ 1 file CSV cho sentiment: script ƒë√£ t√°ch train/test 80/20 v·ªõi stratify=label.

Th·ª±c nghi·ªám

Ph·∫ßn c·ª©ng (s·ª≠a theo m√°y c·ªßa b·∫°n):

GPU: RTX 4090 24GB / TITAN X 12GB

VRAM tham kh·∫£o: CNN/LSTM/Transformer ~ 1‚Äì3 GB; Fine-tune DistilBERT ~ 6‚Äì8 GB

K·∫øt qu·∫£ test (thay b·∫±ng s·ªë th·ª±c t·∫ø):

B√†i to√°n	Size	M√¥ h√¨nh	F1 (t√°i l·∫≠p)	Tham chi·∫øu
ABTE	Small	CNN 1D	0.xx	‚Äì
ABTE	Small	LSTM	0.xx	‚Äì
ABTE	Small	Transformer Encoder	0.xx	‚Äì
ABTE	Small	DistilBERT (ft)	0.xx	‚Äì

C·∫•u h√¨nh/log chi ti·∫øt: out_<model>/logs.txt

Checkpoint/state: out_<model>/checkpoint-XXXX/

Bi·ªÉu ƒë·ªì (F1 / Loss)

Sinh t·ª´ trainer_state.json:

python draw.py


K·∫øt qu·∫£:

compare_f1.png ‚Äî F1 theo epoch

compare_loss.png ‚Äî loss theo epoch

Ch√®n ·∫£nh sau khi commit:

![F1 curves](./compare_f1.png)
![Loss curves](./compare_loss.png)

Demo Streamlit

Ch·∫°y:

streamlit run app.py


App t·ª± d√≤ checkpoint model.safetensors (∆∞u ti√™n out_finetune/checkpoint-*).

C·ªë ƒë·ªãnh th∆∞ m·ª•c:

# trong app.py
ABTE_DIR = "model_abte"  # ch·ª©a model.safetensors + tokenizer


G√≥i t·ªëi thi·ªÉu cho inference (gi·ªØ):

model.safetensors, config.json

tokenizer.json, tokenizer_config.json, special_tokens_map.json

(vocab.txt / merges.txt / spiece.model n·∫øu tokenizer y√™u c·∫ßu)

Xo√° kh·ªèi repo:

optimizer.pt, scheduler.pt, scaler.pt, rng_state.pth, trainer_state.json,
training_args.bin, c√°c .bin/.pt c≈© kh√¥ng c·∫ßn cho inference

C·∫•u tr√∫c d·ª± √°n
ABTE-Sentiment-DL/
‚îú‚îÄ app.py
‚îú‚îÄ train.py
‚îú‚îÄ models.py
‚îú‚îÄ init.py
‚îú‚îÄ draw.py
‚îú‚îÄ requirements.txt
‚îú‚îÄ out_cnn/
‚îú‚îÄ out_lstm/
‚îú‚îÄ out_transformer/
‚îú‚îÄ out_finetune/
‚îî‚îÄ model_abte/
   ‚îú‚îÄ model.safetensors
   ‚îú‚îÄ config.json
   ‚îú‚îÄ tokenizer.json
   ‚îú‚îÄ tokenizer_config.json
   ‚îú‚îÄ special_tokens_map.json
   ‚îî‚îÄ (vocab.txt | merges.txt | spiece.model)

Ghi ch√∫ deploy

B·∫£o m·∫≠t: d√πng safetensors & use_safetensors=True khi load.

Repo n·∫∑ng / push l·ªói 408: chuy·ªÉn l·ªãch s·ª≠ file l·ªõn sang Git LFS
git lfs migrate import --include="*.safetensors,*.pt,*.pth,*.bin,*.h5" --everything
ho·∫∑c ƒë∆∞a model l√™n Hugging Face Hub ƒë·ªÉ GitHub ch·ªâ ch·ª©a code.

.gitignore (g·ª£i √Ω)

__pycache__/
.venv/
**/optimizer.pt
**/scheduler.pt
**/scaler.pt
**/rng_state.pth
**/trainer_state.json
**/training_args.bin
**/pytorch_model.bin
**/*.pt
!**/model.safetensors

Gi·∫•y ph√©p

MIT ho·∫∑c Apache-2.0 (ch·ªçn v√† th√™m file LICENSE).

Ghi c√¥ng

Hugging Face (Transformers/Datasets/Evaluate), SeqEval v√† c√°c b·ªô d·ªØ li·ªáu c√¥ng khai b·∫°n s·ª≠ d·ª•ng.
