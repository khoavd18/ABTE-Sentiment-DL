# ABTE-Sentiment-DL

> **EN / VI bilingual README** ‚Äî Full project README in both **English** and **Ti·∫øng Vi·ªát**.  
> Quick language index:
>
> - [English](#english)
> - [Ti·∫øng Vi·ªát](#ti·∫øng-vi·ªát)

---

## English

### Overview
**ABTE-Sentiment-DL** implements a complete pipeline for:
- **Aspect-Based Term Extraction (ABTE)** at token level with BIO tags (`B-Term`, `I-Term`, `O`).
- **Sentence-level Sentiment Classification** with 3 classes: **Negative / Neutral / Positive**.

Supported models:
- üß± **CNN 1D (Conv1D)**
- üåÄ **LSTM**
- üî∑ **Transformer Encoder (custom)**
- ü§ó **Fine-tune DistilBERT** for **TokenClassification** (ABTE)

A **Streamlit demo** is included. It auto-detects `.safetensors` checkpoints and highlights aspects + predicts sentence/aspect polarities.

---

### Datasets

| Dataset            | Task           | Classes | Train   | Test  |
|--------------------|----------------|:-------:|--------:|------:|
| ABTE-Restaurants   | ABTE (BIO)     |   3     |   3,602 | 1,119 |
| Sentiment-Tweets   | Sentiment (3c) |   3     |  40,000 | 10,000 |

- **ABTE**: columns `Tokens` (`list[str]`), `Tags` (`list[int]` with `O=0, B=1, I=2`).
- **Sentiment**: columns `text`, `label` (`0/1/2` ‚áî `Neg/Neu/Pos`).

> Replace with your sources/links if needed.

---

### Settings
Default unless specified:
- **Epochs**: 100‚ÄÉ(**Early Stopping** `patience=3`)
- **Batch size**: 256 (train/eval)
- **Weight decay**: 0.01
- **Optimizers & LR**
  - CNN/LSTM/Transformer: `Adam(lr=2e-3)`
  - DistilBERT fine-tuning: `AdamW(lr=5e-5)`
- **Tokenizer**: WordLevel (or BPE), trained on train split if applicable
- **Checkpoint**: `save_safetensors=True` ‚Üí produces `model.safetensors`
- **1 epoch** = full pass over data (each sample used exactly once)

---

### Installation
```bash
# 1) Create venv
python -m venv .venv
# 2) Activate
#   Windows PowerShell:
.venv\Scripts\activate
#   Linux/macOS:
# source .venv/bin/activate
# 3) Install deps
pip install -U -r requirements.txt
```

**requirements.txt (suggested)**
```
streamlit>=1.37
transformers>=4.41
tokenizers>=0.15
torch>=2.0
evaluate>=0.4
seqeval==1.2.2
```

---

## Training

### Quick commands (per model)
```bash
# CNN 1D
python train.py --model conv1d        --output_dir out_cnn

# LSTM
python train.py --model lstm          --output_dir out_lstm

# Transformer encoder (custom)
python train.py --model transformer   --output_dir out_transformer

# DistilBERT fine-tuning for ABTE
python train.py --model finetune      --output_dir out_finetune \
  --pretrained distilbert/distilbert-base-uncased
```

### Custom parameters
```bash
python train.py --model conv1d \
  --epochs 20 --batch_size 256 --lr 2e-3 --patience 3 \
  --output_dir out_cnn
```

> If you only have **one CSV** for sentiment, the script splits 80/20 with `stratify=label`.

---

## Experiments
Hardware examples:
- **GPU**: RTX 4060
- **VRAM guide**: CNN/LSTM/Transformer ~ 1‚Äì3 GB; DistilBERT fine-tune ~ 6‚Äì8 GB

**Example test results (replace with your numbers):**

| Task | Size  | Model                 | F1 (reproduced) | Ref |
|------|-------|-----------------------|----------------:|----:|
| ABTE | Small | CNN 1D                | 0.51            |  ‚Äì  |
| ABTE | Small | LSTM                  | 0.37            |  ‚Äì  |
| ABTE | Small | Transformer Encoder   | 0.57            |  ‚Äì  |
| ABTE | Small | **DistilBERT (ft)**   | 0.84            |  ‚Äì  |

- Detailed config/logs per run: `out_<model>/logs.txt`
- Checkpoints/state: `out_<model>/checkpoint-XXXX/`

---

## Curves (F1 / Loss)

Generate curves from `trainer_state.json` (collected in each `out_<model>/checkpoint-*/`):
```bash
python draw.py
```

Artifacts produced:
- `compare_f1.png` ‚Äî evaluation **F1** vs. epoch  
- `compare_loss.png` ‚Äî evaluation **loss** vs. epoch

Embed in README after commit:
```
![F1 Curve](output/compare_f1.png)
![Loss Curve](output/compare_loss.png)
```

---

## Streamlit Demo

Run the demo:
```bash
streamlit run app.py
```

---

### License
MIT or Apache-2.0 (choose one and include a LICENSE file).

### Acknowledgements
Hugging Face (Transformers/Datasets/Evaluate), SeqEval, and the public datasets used.

---

## Ti·∫øng Vi·ªát

### Gi·ªõi thi·ªáu
**ABTE-Sentiment-DL** hi·ªán th·ª±c tr·ªçn v·∫πn:
- **Tr√≠ch aspect (ABTE)** ·ªü m·ª©c token (nh√£n BIO: `B-Term`, `I-Term`, `O`).
- **Ph√¢n lo·∫°i c·∫£m x√∫c c√¢u** v·ªõi 3 l·ªõp: **Ti√™u c·ª±c / Trung t√≠nh / T√≠ch c·ª±c**.

H·ªó tr·ª£ c√°c m√¥ h√¨nh:
- üß± **CNN 1D (Conv1D)**
- üåÄ **LSTM**
- üî∑ **Transformer Encoder (t√πy bi·∫øn)**
- ü§ó **Fine-tune DistilBERT** cho **TokenClassification** (ABTE)

C√≥ **demo Streamlit** t·ª± d√≤ checkpoint `.safetensors`, t√¥ m√†u aspect v√† d·ª± ƒëo√°n c·∫£m x√∫c c√¢u/cho t·ª´ng aspect.

---

### D·ªØ li·ªáu

| Dataset            | B√†i to√°n      | S·ªë l·ªõp | Train   | Test  |
|--------------------|---------------|:------:|--------:|------:|
| ABTE-Restaurants   | ABTE (BIO)    |   3    |   3,602 | 1,119 |
| Sentiment-Tweets   | C·∫£m x√∫c (3c)  |   3    |  40,000 | 10,000 |

- **ABTE**: c·ªôt `Tokens` (`list[str]`), `Tags` (`list[int]` v·ªõi `O=0, B=1, I=2`).
- **Sentiment**: c·ªôt `text`, `label` (`0/1/2` ‚áî `Ti√™u c·ª±c/Trung t√≠nh/T√≠ch c·ª±c`).

---

### Thi·∫øt l·∫≠p
M·∫∑c ƒë·ªãnh (tr·ª´ khi n√™u kh√°c):
- **Epochs**: 100‚ÄÉ(**Early Stopping** `patience=3`)
- **Batch**: 256 (train/eval)
- **Weight decay**: 0.01
- **T·ªëi ∆∞u & LR**
  - CNN/LSTM/Transformer: `Adam(lr=2e-3)`
  - Fine-tune DistilBERT: `AdamW(lr=5e-5)`
- **Tokenizer**: WordLevel (ho·∫∑c BPE)
- **Checkpoint**: `save_safetensors=True` ‚Üí sinh `model.safetensors`
- **1 epoch** = duy·ªát h·∫øt d·ªØ li·ªáu (m·ªói m·∫´u d√πng ƒë√∫ng 1 l·∫ßn)

---

## Hu·∫•n luy·ªán

### L·ªánh nhanh theo m√¥ h√¨nh
```bash
# CNN 1D
python train.py --model conv1d        --output_dir out_cnn

# LSTM
python train.py --model lstm          --output_dir out_lstm

# Transformer encoder
python train.py --model transformer   --output_dir out_transformer

# Fine-tune DistilBERT cho ABTE
python train.py --model finetune      --output_dir out_finetune \
  --pretrained distilbert/distilbert-base-uncased
```

### Tu·ª≥ bi·∫øn tham s·ªë
```bash
python train.py --model conv1d \
  --epochs 20 --batch_size 256 --lr 2e-3 --patience 3 \
  --output_dir out_cnn
```

> N·∫øu ch·ªâ c√≥ **1 file CSV** cho sentiment: script s·∫Ω t√°ch `train/test` 80/20 v·ªõi `stratify=label`.

---

## Th·ª±c nghi·ªám
Ph·∫ßn c·ª©ng:
- **GPU**: RTX 4090 24GB / TITAN X 12GB
- **VRAM tham kh·∫£o**: CNN/LSTM/Transformer ~ 1‚Äì3 GB; Fine-tune DistilBERT ~ 6‚Äì8 GB

**K·∫øt qu·∫£ test (thay b·∫±ng s·ªë th·ª±c t·∫ø):**

| B√†i to√°n | Size  | M√¥ h√¨nh               | F1 (t√°i l·∫≠p) | Tham chi·∫øu |
|----------|-------|-----------------------|-------------:|-----------:|
| ABTE     | Small | CNN 1D                | 0.xx         |     ‚Äì      |
| ABTE     | Small | LSTM                  | 0.xx         |     ‚Äì      |
| ABTE     | Small | Transformer Encoder   | 0.xx         |     ‚Äì      |
| ABTE     | Small | **DistilBERT (ft)**   | **0.xx**     |     ‚Äì      |

- C·∫•u h√¨nh/log chi ti·∫øt: `out_<model>/logs.txt`  
- Checkpoint/state: `out_<model>/checkpoint-XXXX/`

---

## Bi·ªÉu ƒë·ªì (F1 / Loss)

Sinh t·ª´ `trainer_state.json` (n·∫±m trong t·ª´ng `out_<model>/checkpoint-*/`):
```bash
python draw.py
```

File k·∫øt qu·∫£:
- `compare_f1.png` ‚Äî **F1** theo epoch  
- `compare_loss.png` ‚Äî **loss** theo epoch

Ch√®n v√†o README:
```md
![F1 curves](./compare_f1.png)
![Loss curves](./compare_loss.png)
```

---

## Demo Streamlit

Ch·∫°y demo:
```bash
streamlit run app.py
```



---

### Gi·∫•y ph√©p
MIT ho·∫∑c Apache-2.0 (ch·ªçn v√† th√™m file `LICENSE`).

### Ghi c√¥ng
Hugging Face (Transformers/Datasets/Evaluate), SeqEval v√† c√°c b·ªô d·ªØ li·ªáu c√¥ng khai b·∫°n s·ª≠ d·ª•ng.
